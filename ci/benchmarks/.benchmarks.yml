.use_base_container: &use_base_container
  stage: benchmarks
  image: "${CI_REGISTRY_IMAGE}:latest"
  services:
    - name: docker:dind
      alias: docker

.run_benchmarks: &run_benchmarks
  <<: *use_base_container
  before_script:
    - pip3 install -e .
    - pip install litellm langchain transformers torch openai tqdm
    - |
      for var in $(compgen -e); do
        if [[ $var == CTF_* && -n ${!var} ]]; then
          export $var="${!var}"
        fi
      done
    # - curl http://localhost:8000/api/tags  #Â validate that ollama is running
  variables:
    OLLAMA_API_BASE: "http://localhost:8000"
    OPENROUTER_API_BASE: "https://openrouter.ai/api/v1"
    OPENAI_API_BASE: "https://api.openai.com/v1"
    OPENAI_API_KEY: $OPENAI_API_KEY
  artifacts:
    paths:
      - cai/benchmarks/seceval/eval/outputs/
      - cai/benchmarks/cybermetric/outputs/
    expire_in: 12 month
  tags:
    - p40
    - x86
  rules:
    - if: $CI_COMMIT_BRANCH
      when: on_success

# benchmarks-test-seceval:
#   <<: *run_benchmarks
#   variables:
#     OLLAMA_API_BASE: "http://localhost:8000"
#     OPENROUTER_API_BASE: "https://openrouter.ai/api/v1"
#     OPENAI_API_KEY: "fake-api-key"
#   script:
#     - pip3 install -e .
#     - pip install -r benchmarks/seceval/eval/requirements.txt
#     - python3 benchmarks/seceval/eval/eval.py --dataset_file benchmarks/seceval/eval/datasets/questions-2.json --output_dir benchmarks/seceval/eval/outputs --backend ollama --models ollama/qwen2.5:14b

benchmarks-test-cybermetric:
  <<: *run_benchmarks
  script:
    - python3 benchmarks/cybermetric/CyberMetric_evaluator.py --model_name ollama/qwen2.5:14b --file_path benchmarks/cybermetric/CyberMetric-2-v1.json

benchmarks-test-cybermetric-openai:
  <<: *run_benchmarks
  script:
    - python3 benchmarks/cybermetric/CyberMetric_evaluator.py --model_name openai/gpt-4o --file_path benchmarks/cybermetric/CyberMetric-2-v1.json
