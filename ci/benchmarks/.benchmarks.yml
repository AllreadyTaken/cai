.use_base_container: &use_base_container
  stage: benchmarks
  image: "${CI_REGISTRY_IMAGE}:latest"
  services:
    - name: docker:dind
      alias: docker

.run_benchmarks: &run_benchmarks
  <<: *use_base_container
  script:
    - pip3 install -e .
    - pip install litellm langchain transformers torch openai tqdm
    - echo "Checking environment variables..."

    - |
      for var in $(compgen -e); do
        if [[ ($var == CTF_* || $var == *_API_KEY || $var == *_API_BASE) && -n ${!var} ]]; then
          export $var="${!var}"          
        fi
      done
    
    - python3 benchmarks/cybermetric/CyberMetric_evaluator.py --model_name $MODEL_NAME --file_path $BENCHMARK_FILE
    - pwd
    - ls -la
    # - curl http://localhost:8000/api/tags  # validate that ollama is running
  variables:
    OLLAMA_API_BASE: "http://localhost:8000"
    OPENROUTER_API_BASE: "https://openrouter.ai/api/v1"
    OPENAI_API_BASE: "https://api.openai.com/v1"
  artifacts:
    paths:
      - benchmarks/outputs/
      - benchmarks/cybermetric/outputs
    expire_in: 12 month
  tags:
    - p40
    - x86
  rules:
    - if: $CI_COMMIT_BRANCH
      when: on_success

# benchmarks-test-seceval:
#   <<: *run_benchmarks
#   variables:
#     OLLAMA_API_BASE: "http://localhost:8000"
#     OPENROUTER_API_BASE: "https://openrouter.ai/api/v1"
#     OPENAI_API_KEY: "fake-api-key"
#   script:
#     - pip3 install -e .
#     - pip install -r benchmarks/seceval/eval/requirements.txt
#     - python3 benchmarks/seceval/eval/eval.py --dataset_file benchmarks/seceval/eval/datasets/questions-2.json --output_dir benchmarks/seceval/eval/outputs --backend ollama --models ollama/qwen2.5:14b

benchmarks-test-cybermetric-openrouter:
  <<: *run_benchmarks
  variables:
    MODEL_NAME: "openrouter/qwen/qwen3-32b:free"
    BENCHMARK_FILE: "benchmarks/cybermetric/CyberMetric-2-v1.json"

benchmarks-test-cybermetric-openai:
  <<: *run_benchmarks
  variables:
    MODEL_NAME: "openai/gpt-4o-mini"
    BENCHMARK_FILE: "benchmarks/cybermetric/CyberMetric-2-v1.json"
